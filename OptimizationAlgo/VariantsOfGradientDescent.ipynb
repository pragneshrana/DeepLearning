{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VariantsOfGradientDescent.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOfNDTy0aklV69sjaZyKpUq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragneshrana/DeepLearning/blob/master/OptimizationAlgo/VariantsOfGradientDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2MchwmwdKrZ",
        "colab_type": "text"
      },
      "source": [
        "**Gradient Descent Update Rule**\n",
        "\n",
        "$W += \\eta \\frac{\\partial \\mathcal{L}}{\\partial W}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqqXiIusejf9",
        "colab_type": "text"
      },
      "source": [
        "**Is there any better rule than this ?**\n",
        "\n",
        "**Can we do better with it?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSdPdjH1lbWW",
        "colab_type": "text"
      },
      "source": [
        "# Momentum Based GD\n",
        "\n",
        "- like if you are constaly taking small steps then it will take lot of time to converge.\n",
        "\n",
        "- so, while moving in certain direction with confience, it is good idea to increment step size dynamically.\n",
        "\n",
        "- Keep that rule GD update rule will be,\n",
        "\n",
        "$\\nu_t = \\underbrace{\\gamma \\nu_{t+1}}_{History} + \\eta \\Delta W = \\underbrace{\\gamma^{t-1} \\eta \\Delta W_1+ \\gamma^{t-2} \\eta \\Delta W_2+ \\gamma^{t-3} \\eta \\Delta W_3 + ... }_{History} + \\eta \\Delta W$\n",
        "\n",
        "$W_{t+1} = W_t - \\nu_t$\n",
        "\n",
        "- In history exponantial decaing weight sum is used. More older the history, less weight is given.\n",
        "\n",
        "Advantage:\n",
        "- Converge is faster\n",
        "- Helps to escape platu region faster\n",
        "\n",
        "Disadvantage:\n",
        "- Overshoot and oscillation around the destination/minima"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WnGraqFxNll",
        "colab_type": "text"
      },
      "source": [
        "# Nesterov Accelerated Gradient Descent\n",
        "\n",
        "- Rether than using gradient of previous step, use gradient of most rescent step.\n",
        "\n",
        "$\\nabla W_{temp} = W_t + \\gamma \\nu_{t-1}$\n",
        "\n",
        "$W_{t+1} = W_{temp} + \\eta \\nabla W_{temp}$\n",
        "\n",
        "$\\nu_{t} = \\gamma \\nu_{t-1}+ \\eta \\nabla W_{temp}$\n",
        "\n",
        "\n",
        "Advantage :\n",
        "- overshoot will be lesser\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkoXrPKH3iUI",
        "colab_type": "text"
      },
      "source": [
        "# How do we computer gradients? How much data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWklCgjk4oS_",
        "colab_type": "text"
      },
      "source": [
        "- $\\nabla W = \\frac{\\partial \\mathcal{L}}{\\partial W} = \\sum_i^n\\frac{\\partial}{\\partial W} (f(x_i) -y_i)^2 $\n",
        "\n",
        "- For large dataset computation of gradient will take lot of time.\n",
        "-  This happens as we are using all the data points\n",
        "- What is we use random fewer data points?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ISAkUd76MAB",
        "colab_type": "text"
      },
      "source": [
        "# Vanila (Batch) GD (steps:N)\n",
        "  - Single batch \n",
        "  - Time consuming :( \n",
        "\n",
        "# Stochastic GD  (steps:1)\n",
        "  - one data point at a time\n",
        "  - Quicker update :)\n",
        "  - Approximateupdate :(\n",
        "\n",
        "# Mini-Batch GD  (steps:N/B)\n",
        "  - Update after set of random data points\n",
        "  - Osciallation decreases :)\n",
        "  - Appromation increases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJCoAgwY-K0a",
        "colab_type": "text"
      },
      "source": [
        "# Better leraning Algo \n",
        " - Adam \n",
        " - RMSPROP\n",
        " - Ada Grad\n",
        "\n",
        "- Adaptive learning rate \n",
        "    - Sparse Matrix - larger learning rate\n",
        "    - Dense Matrix - low learning rate\n",
        "\n",
        "# Ada Grad\n",
        "\n",
        "$\\nu_t = \\nu_{t-1} + (\\nabla w_t)^2$\\\n",
        "$w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\nu_t}+\\epsilon} + \\nabla w_t$\n",
        "\n",
        "- $\\epsilon$ for non-zero denominator\n",
        "- For feature which frequently gets updated (dense feauture) learning rate will be decreased. Opposite for sparse feature.\n",
        "\n",
        "- **Disadvantage:**\n",
        "  - If denominator grows rapidly learning rate $\\rightarrow 0$.\n",
        "\n",
        "\n",
        "# RMSProp\n",
        "\n",
        "$\\nu_t = \\beta \\nu_{t-1} +(1-\\beta) (\\nabla w_t)^2$\\\n",
        "$w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\nu_t}+\\epsilon} + \\nabla w_t$\n",
        "\n",
        "- $\\beta$ to reduce the decay\n",
        "\n",
        "# Adam\n",
        "$m_t = \\beta_1 \\nu_{t-1} +(1-\\beta_1) (\\nabla w_t)$ # Momentum based history\n",
        "\n",
        "$m_t = \\frac{m_t}{1-\\beta_1^t}$ #Bias coorection \n",
        "\n",
        "$\\nu_t = \\beta_2 \\nu_{t-1} +(1-\\beta_2) (\\nabla w_t)^2$ # decaying learning rate\n",
        "\n",
        "$\\nu_t = \\frac{\\nu_t}{1-\\beta_2^t}$ # Bias correction \n",
        "\n",
        "$w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\nu_t}+\\epsilon} + \\nabla w_t$ \n",
        "\n",
        "- It combines momentum based gradint descent and RMSProp update rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AljKwLAnJxVT",
        "colab_type": "text"
      },
      "source": [
        "# Adam + Mini Batch :) Have Trust"
      ]
    }
  ]
}