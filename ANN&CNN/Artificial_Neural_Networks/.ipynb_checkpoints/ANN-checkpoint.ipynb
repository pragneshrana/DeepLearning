{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pragneshrana/DeepLearning/blob/master/Artificial_Neural_Networks/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxAXC8rwjZBQ"
   },
   "source": [
    "Topics:\n",
    "1. The Neuron \n",
    "2. The Activation Function\n",
    "3. How neural network works and learn?\n",
    "4. Gradient descent \n",
    "5. Stochastic Gradient Descent \n",
    "6. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJsrrn4ZjkXs"
   },
   "source": [
    "#1. Neuron \n",
    "Folowing process takes place in neuron \\\n",
    "  I. Take input values \\\n",
    "  II. Weighted sum of input values \\\n",
    "  III. Functional operation over weighted sum (Activation Function) \\\n",
    "  IV. Output \\\n",
    "\n",
    "#2. The Activation Fucntion \n",
    "Activation function trasforms the weighted sum by measn of nonlinear function.\\\n",
    "# Why we need non linearirty? \n",
    "By adding non-linearity by means of fucntion, neurons can learn complex task by polynomial kind of fitting.\n",
    "\n",
    "#3. How neural network works and learn?\n",
    "Wighted sum of input is passed to the neurons which get transformed by activation fucntion and generates the output. Similar happens for every neurons.\\\n",
    "If final output is wrong then backpropagation step will adjust the weights. Again feed of input will take place and predict the result till final result is not obtained (error is not minimized). \n",
    "\n",
    "#4. Gradient descent\n",
    "Each neuron receives the linear combination of inputs from previous neurons which alike multilienar regressiona in which we have to find out the weights by linear combination. Such optimized weights can be found out by gradient descent.If you are updating weights using all data points then it called as 'Batch' Gradient descent.<br>\n",
    "\n",
    "\n",
    "If you are updating weights by \n",
    "$h(x)= \\beta^TX = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2+...+\\beta_nx_n$<br>\n",
    "for m data points<br>,\n",
    "Cost fucntion: $J = \\frac{1}{2m} \\sum_{i=1}^{m}(h(x^{i}) - y^{i})^2$\n",
    "\n",
    "Algo: <br>\n",
    "Assume the parameter and increment by updating in each iteration,<br>\n",
    "$\\beta_j :=  \\beta_j - \\alpha \\dfrac{ \\partial}{\\partial \\beta_j} J(\\beta_0,\\beta_1,\\beta_2,...,\\beta_n)$\n",
    "<br>\n",
    "\n",
    "By which we can obtain,<br>\n",
    "$\\beta_j = \\beta_j - \\alpha \\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_j^{(i)}$<br>\n",
    "Here, for $i^{th}$ denotes data point and $j$ denotes features or columns<br> \n",
    "\n",
    "#4. Stochastic Gradient Descent \n",
    "If weights are updated using randomly selecting each datapoint then it called as stochastic graient descent<br>\n",
    "Ref : https://towardsdatascience.com/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843\n",
    "\n",
    "\n",
    "#5. Backpropagation\n",
    "REF: https://remykarem.github.io/backpropagation-demo/ <br>\n",
    "REF: https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba <br>\n",
    "Visulization and play tool <br>\n",
    "In forward propogation by linear combination of input and functional tranfrmation finally $\\hat y$ is obtained.<br>\n",
    "\n",
    "for the first hidden layer of neural network,<br>\n",
    "\n",
    "$p_1 =f(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2+ ...+ \\beta_n x_n)$<br>\n",
    "$p_2 =f(\\gamma_0 + \\gamma_1 x_1 + \\gamma_2 x_2+ ...+ \\gamma_n x_n)$<br>\n",
    "$\\vdots$<br>\n",
    "$p_n =f(\\alpha_0 + \\alpha_1 x_1 + \\alpha_2 x_2+ ...+ \\alpha_n x_n)$<br>\n",
    "where,f denotes the sigmoid/ ReLU fucntion function \n",
    "\n",
    "for the second or output layer <br>\n",
    "$\\hat y=g(\\beta_0 + \\beta_1 p_1 + \\beta_2 p_2+ ...+ \\beta_n p_n)$<br>\n",
    "where g denotes the sigmoid/ ReLU fucntion <br>\n",
    "\n",
    "for example,<br>]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyeJru0qjk0I"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHxysLQBfcg6"
   },
   "outputs": [],
   "source": [
    "#Artificial Neural Network\n",
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/pragneshrana/DeepLearning/master/Artificial_Neural_Networks/Churn_Modelling.csv'\n",
    "dataset = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "yiCDHY28gJK1",
    "outputId": "0f7a0c0b-7693-427b-cccd-8daefbcc1b2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "#Importing the dataset \n",
    "X = dataset.iloc[:,3:13].values\n",
    "y = dataset.iloc[:,13].values\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "7ZyqvKNzZR0k",
    "outputId": "174fc82e-b9f6-4180-81a0-9d5f46531c97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#encoding the variable\n",
    "#label  encoder converts the categoical data into numeric data with same number of categories\n",
    "#whereas onehot encoder generated number of rows by taking combination of rows.\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "geographic_encoder_X_1 = LabelEncoder()  #conveert into numeric categories\n",
    "X[:,1] = geographic_encoder_X_1.fit_transform(X[:,1])\n",
    "\n",
    "gender_encoder_X_2 = LabelEncoder()\n",
    "X[:,2] = gender_encoder_X_2.fit_transform(X[:,2])\n",
    "\n",
    "#convert numeric categories into colm combination\n",
    "ct = ColumnTransformer(\n",
    "    [('one_hot_encoder', #name\n",
    "      OneHotEncoder(), #TransformClass\n",
    "      [1])],    #index to transform\n",
    "    remainder='passthrough'                         # Leave the rest of the columns untouched\n",
    ")\n",
    "\n",
    "X = np.array(ct.fit_transform(X), dtype=np.float)\n",
    "X = X[:,1:] #REMOVED FIRST COLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OVOM8FiZYdc"
   },
   "outputs": [],
   "source": [
    "#Train Test set \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DzTcS4N1c0qk"
   },
   "outputs": [],
   "source": [
    "#feature scaling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(with_mean=False)\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test  = sc.transform(X_test)\n",
    "\n",
    "#TO see the diffn\n",
    "# scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "# scaler.transform(X_train) \n",
    "# scaler.transform(X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "fZWJhVF2flL-",
    "outputId": "64a42631-0910-4fef-ae18-5c4f2f7fe470"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Keras and Implementation of ANN\n",
    "import keras\n",
    "from keras.models import Sequential #to initialize the NN\n",
    "from keras.layers import  Dense #to generate the layers\n",
    "from keras import dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "colab_type": "code",
    "id": "kXiqBbgXgZjO",
    "outputId": "b15c6197-4f51-49ed-f1a2-fcec60c730a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#Initialize the NN\n",
    "classifier = Sequential() #to initialize the first layer\n",
    "\n",
    "#Adding the input layer and the first hidden layer \n",
    "\n",
    "#Hidden layer\n",
    "#sigmoid for output layer \n",
    "#Rectifier for Hidden layer\n",
    "classifier.add(Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=11, units=6))\n",
    "\n",
    "#how many nodes? No rules but (avg of number nodes input and output layer) #input+output/2  =(11+1)/2 = 6\n",
    "#init  = initilizatin of values\n",
    "\n",
    "#second hidden layer \n",
    "classifier.add(Dropout(p=0.1)) #to avoid overfitting #start with low value if ovefitting doesn't reduce do ovefitting\n",
    "classifier.add(Dense(units=6, activation=\"relu\", kernel_initializer=\"uniform\")) #netwrok knows how to connect so need of input_dim\n",
    "\n",
    "\n",
    "#Add final-output layer\n",
    "classifier.add(Dropout(p=0.1))  #to avoid overfitting\n",
    "classifier.add(Dense(units=1, activation=\"sigmoid\", kernel_initializer=\"uniform\")) #netwrok knows how to connect so need of input_dim\n",
    "\n",
    "#compile the ANN\n",
    "classifier.compile(optimizer ='adam', loss ='binary_crossentropy',metrics = ['accuracy'])\n",
    "#adam is stochastic gradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vEvipYFbhZE3",
    "outputId": "c2372f9d-cf6d-481d-8267-3735bbc76d6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/70\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "8000/8000 [==============================] - 4s 442us/step - loss: 0.5215 - acc: 0.7956\n",
      "Epoch 2/70\n",
      "8000/8000 [==============================] - 2s 292us/step - loss: 0.4777 - acc: 0.7960\n",
      "Epoch 3/70\n",
      "8000/8000 [==============================] - 2s 305us/step - loss: 0.4576 - acc: 0.7960\n",
      "Epoch 4/70\n",
      "8000/8000 [==============================] - 2s 304us/step - loss: 0.4486 - acc: 0.7960\n",
      "Epoch 5/70\n",
      "8000/8000 [==============================] - 2s 296us/step - loss: 0.4444 - acc: 0.7960\n",
      "Epoch 6/70\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 0.4421 - acc: 0.7960\n",
      "Epoch 7/70\n",
      "8000/8000 [==============================] - 2s 294us/step - loss: 0.4407 - acc: 0.7975\n",
      "Epoch 8/70\n",
      "8000/8000 [==============================] - 2s 296us/step - loss: 0.4392 - acc: 0.8057\n",
      "Epoch 9/70\n",
      "8000/8000 [==============================] - 2s 305us/step - loss: 0.4393 - acc: 0.8087\n",
      "Epoch 10/70\n",
      "8000/8000 [==============================] - 2s 309us/step - loss: 0.4390 - acc: 0.8082\n",
      "Epoch 11/70\n",
      "8000/8000 [==============================] - 2s 306us/step - loss: 0.4381 - acc: 0.8079\n",
      "Epoch 12/70\n",
      "8000/8000 [==============================] - 2s 303us/step - loss: 0.4369 - acc: 0.8096\n",
      "Epoch 13/70\n",
      "8000/8000 [==============================] - 2s 301us/step - loss: 0.4368 - acc: 0.8102\n",
      "Epoch 14/70\n",
      "8000/8000 [==============================] - 2s 302us/step - loss: 0.4362 - acc: 0.8086\n",
      "Epoch 15/70\n",
      "8000/8000 [==============================] - 2s 301us/step - loss: 0.4358 - acc: 0.8107\n",
      "Epoch 16/70\n",
      "8000/8000 [==============================] - 2s 306us/step - loss: 0.4352 - acc: 0.8117\n",
      "Epoch 17/70\n",
      "8000/8000 [==============================] - 2s 299us/step - loss: 0.4347 - acc: 0.8111\n",
      "Epoch 18/70\n",
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4336 - acc: 0.8134\n",
      "Epoch 19/70\n",
      "8000/8000 [==============================] - 2s 301us/step - loss: 0.4332 - acc: 0.8126\n",
      "Epoch 20/70\n",
      "8000/8000 [==============================] - 2s 292us/step - loss: 0.4324 - acc: 0.8152\n",
      "Epoch 21/70\n",
      "8000/8000 [==============================] - 2s 299us/step - loss: 0.4316 - acc: 0.8135\n",
      "Epoch 22/70\n",
      "8000/8000 [==============================] - 2s 288us/step - loss: 0.4309 - acc: 0.8162\n",
      "Epoch 23/70\n",
      "8000/8000 [==============================] - 2s 296us/step - loss: 0.4299 - acc: 0.8164\n",
      "Epoch 24/70\n",
      "8000/8000 [==============================] - 2s 294us/step - loss: 0.4290 - acc: 0.8180\n",
      "Epoch 25/70\n",
      "8000/8000 [==============================] - 2s 290us/step - loss: 0.4275 - acc: 0.8199\n",
      "Epoch 26/70\n",
      "8000/8000 [==============================] - 2s 312us/step - loss: 0.4267 - acc: 0.8226\n",
      "Epoch 27/70\n",
      "8000/8000 [==============================] - 2s 303us/step - loss: 0.4261 - acc: 0.8229\n",
      "Epoch 28/70\n",
      "8000/8000 [==============================] - 2s 295us/step - loss: 0.4249 - acc: 0.8216\n",
      "Epoch 29/70\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 0.4247 - acc: 0.8247\n",
      "Epoch 30/70\n",
      "8000/8000 [==============================] - 2s 305us/step - loss: 0.4243 - acc: 0.8229\n",
      "Epoch 31/70\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 0.4235 - acc: 0.8225\n",
      "Epoch 32/70\n",
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4233 - acc: 0.8239\n",
      "Epoch 33/70\n",
      "8000/8000 [==============================] - 2s 290us/step - loss: 0.4220 - acc: 0.8229\n",
      "Epoch 34/70\n",
      "8000/8000 [==============================] - 2s 295us/step - loss: 0.4215 - acc: 0.8250\n",
      "Epoch 35/70\n",
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4213 - acc: 0.8257\n",
      "Epoch 36/70\n",
      "8000/8000 [==============================] - 2s 290us/step - loss: 0.4203 - acc: 0.8242\n",
      "Epoch 37/70\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 0.4191 - acc: 0.8262\n",
      "Epoch 38/70\n",
      "8000/8000 [==============================] - 2s 299us/step - loss: 0.4191 - acc: 0.8264\n",
      "Epoch 39/70\n",
      "8000/8000 [==============================] - 2s 307us/step - loss: 0.4188 - acc: 0.8260\n",
      "Epoch 40/70\n",
      "8000/8000 [==============================] - 2s 295us/step - loss: 0.4175 - acc: 0.8281\n",
      "Epoch 41/70\n",
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4174 - acc: 0.8286\n",
      "Epoch 42/70\n",
      "8000/8000 [==============================] - 2s 290us/step - loss: 0.4164 - acc: 0.8294\n",
      "Epoch 43/70\n",
      "8000/8000 [==============================] - 2s 304us/step - loss: 0.4163 - acc: 0.8281\n",
      "Epoch 44/70\n",
      "8000/8000 [==============================] - 2s 299us/step - loss: 0.4161 - acc: 0.8285\n",
      "Epoch 45/70\n",
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4153 - acc: 0.8279\n",
      "Epoch 46/70\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 0.4149 - acc: 0.8275\n",
      "Epoch 47/70\n",
      "8000/8000 [==============================] - 2s 297us/step - loss: 0.4147 - acc: 0.8291\n",
      "Epoch 48/70\n",
      "8000/8000 [==============================] - 2s 302us/step - loss: 0.4133 - acc: 0.8290\n",
      "Epoch 49/70\n",
      "8000/8000 [==============================] - 2s 299us/step - loss: 0.4133 - acc: 0.8295\n",
      "Epoch 50/70\n",
      "8000/8000 [==============================] - 2s 304us/step - loss: 0.4135 - acc: 0.8289\n",
      "Epoch 51/70\n",
      "8000/8000 [==============================] - 2s 300us/step - loss: 0.4129 - acc: 0.8325\n",
      "Epoch 52/70\n",
      "8000/8000 [==============================] - 2s 296us/step - loss: 0.4118 - acc: 0.8314\n",
      "Epoch 53/70\n",
      "8000/8000 [==============================] - 2s 297us/step - loss: 0.4120 - acc: 0.8299\n",
      "Epoch 54/70\n",
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4120 - acc: 0.8277\n",
      "Epoch 55/70\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 0.4112 - acc: 0.8305\n",
      "Epoch 56/70\n",
      "8000/8000 [==============================] - 2s 301us/step - loss: 0.4108 - acc: 0.8309\n",
      "Epoch 57/70\n",
      "8000/8000 [==============================] - 2s 299us/step - loss: 0.4104 - acc: 0.8312\n",
      "Epoch 58/70\n",
      "8000/8000 [==============================] - 2s 299us/step - loss: 0.4106 - acc: 0.8296\n",
      "Epoch 59/70\n",
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4111 - acc: 0.8291\n",
      "Epoch 60/70\n",
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4106 - acc: 0.8300\n",
      "Epoch 61/70\n",
      "8000/8000 [==============================] - 2s 297us/step - loss: 0.4098 - acc: 0.8314\n",
      "Epoch 62/70\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 0.4097 - acc: 0.8309\n",
      "Epoch 63/70\n",
      "8000/8000 [==============================] - 2s 291us/step - loss: 0.4095 - acc: 0.8295\n",
      "Epoch 64/70\n",
      "8000/8000 [==============================] - 2s 296us/step - loss: 0.4098 - acc: 0.8317\n",
      "Epoch 65/70\n",
      "8000/8000 [==============================] - 2s 290us/step - loss: 0.4093 - acc: 0.8302\n",
      "Epoch 66/70\n",
      "8000/8000 [==============================] - 2s 297us/step - loss: 0.4087 - acc: 0.8317\n",
      "Epoch 67/70\n",
      "8000/8000 [==============================] - 2s 295us/step - loss: 0.4085 - acc: 0.8301\n",
      "Epoch 68/70\n",
      "8000/8000 [==============================] - 2s 300us/step - loss: 0.4086 - acc: 0.8310\n",
      "Epoch 69/70\n",
      "8000/8000 [==============================] - 2s 294us/step - loss: 0.4092 - acc: 0.8311\n",
      "Epoch 70/70\n",
      "8000/8000 [==============================] - 2s 295us/step - loss: 0.4083 - acc: 0.8320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08dcf92a58>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model to training set\n",
    "classifier.fit(X_train,y_train,batch_size = 10,epochs = 70)\n",
    "# 3. Update weights \n",
    "# 4. num of epochs                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWFIksB5qgTF"
   },
   "outputs": [],
   "source": [
    "#Predicction\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "iXvYVX3frSta",
    "outputId": "f9880c22-71cf-404d-857d-9cb3576df42e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1565   30]\n",
      " [ 296  109]]\n"
     ]
    }
   ],
   "source": [
    "#Making the confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQ5iwY6BsATM"
   },
   "outputs": [],
   "source": [
    "#Homework\n",
    "data = {'Geography_France':1,\n",
    "        'Geography_Germany':0,\n",
    "        'Geography_Spain':0,\n",
    "        'Credit_Score':600.0,\n",
    "        'Gender':'Male',\n",
    "        'Age':40,\n",
    "        'Tenure':3,\n",
    "        'Balance':60000,\n",
    "        'NO_of_prod':2,\n",
    "        'Credit_card':0,\n",
    "        'Active_Mem':1,\n",
    "        'Salary':50000}\n",
    "data_feature = np.array([list(data.values())])\n",
    "\n",
    "#encoding\n",
    "data_feature[:,4] = gender_encoder_X_2.transform(data_feature[:,4])\n",
    "data_feature = data_feature[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-1glawfX4EHl",
    "outputId": "65bdbda3-4ab0-44fa-8590-dddebba29ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer will leave bank or not? [[False]]\n"
     ]
    }
   ],
   "source": [
    "new_prediction = classifier.predict(data_feature) \n",
    "new_prediction = (new_prediction > 0.5)\n",
    "print('Customer will leave bank or not?',new_prediction)\n",
    "#info in horizonatal array as observation are in row \n",
    "#two brackets as original data is in matrix form and we are just adding one row in matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "id": "vwnf7CLpBH8G",
    "outputId": "6f93c9b8-1d3a-4d58-ecb4-cc6287c6791f"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-77fc6da66255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m \u001b[0;31m#to initialize the NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mDense\u001b[0m \u001b[0;31m#to generate the layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'dropout'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Evaluation of ANN by k fold cross validaion\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import keras\n",
    "from keras.models import Sequential #to initialize the NN\n",
    "from keras.layers import  Dense #to generate the layers\n",
    "\n",
    "def build_classifier():\n",
    "    '''\n",
    "    LOCAL CLASSIFIER\n",
    "    for cross validation K times going to call this function \n",
    "    and it will return \n",
    "    '''\n",
    "    #Initialize the NN\n",
    "    classifier = Sequential() #to initialize the first layer\n",
    "\n",
    "    #Adding the input layer and the first hidden layer \n",
    "\n",
    "    #Hidden layer\n",
    "    #sigmoid for output layer \n",
    "    #Rectifier for Hidden layer\n",
    "    classifier.add(Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=11, units=6))\n",
    "    #how many nodes? No rules but (avg of number nodes input and output layer) #input+output/2  =(11+1)/2 = 6\n",
    "    #init  = initilizatin of values\n",
    "\n",
    "    #second hidden layer \n",
    "    classifier.add(Dense(units=6, activation=\"relu\", kernel_initializer=\"uniform\")) #netwrok knows how to connect so need of input_dim\n",
    "\n",
    "    #Add final-output layer \n",
    "    classifier.add(Dense(units=1, activation=\"sigmoid\", kernel_initializer=\"uniform\")) #netwrok knows how to connect so need of input_dim\n",
    "\n",
    "    #compile the ANN\n",
    "    classifier.compile(optimizer ='adam', loss ='binary_crossentropy',metrics = ['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_classifier,batch_size=10,nb_epoch=100)\n",
    "accu = cross_val_score(estimator = classifier, X = X_train, y = y_train,cv=10)\n",
    "# ss_val_score(estimator =classifier, x=X_train, y=y_train, cv =10, n_jobsn = -1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "GyiZblMKH3-h",
    "outputId": "b4f9cc52-e5b2-4785-cf6e-e6770b90c5dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7959999969601632\n",
      "0.010105690620981692\n"
     ]
    }
   ],
   "source": [
    "mean = accu.mean()\n",
    "print(mean)\n",
    "variance = accu.std()\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G3ZFF65_Ivyn"
   },
   "outputs": [],
   "source": [
    "#### Improving ANN\n",
    "#Drop Regularization\n",
    "#Drop out to avoid the overfitting the ANN \n",
    "#Some correlation are not useful which has high variance so to avois scenario this method is useful\n",
    "\n",
    "##How it works?\n",
    "# At each iteration of the training some neurons will be randomly disabled to prevent them to learn \n",
    "# dependent correlation which force neurons to learn independent features which prevents neurons to \n",
    "# learning too much which prevents overfitting.\n",
    "\n",
    "\n",
    "#### Parameter Tuning\n",
    "# Hyper parameters which are fixed during the computation\n",
    "# USING GRID SEARCH METHOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "16oF7w_xPe1E",
    "outputId": "ff844864-5db6-4929-f7a3-66cb0c1c4e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 2s 292us/step - loss: 0.5434 - acc: 0.7971\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 2s 304us/step - loss: 0.5567 - acc: 0.7967\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 2s 295us/step - loss: 0.5539 - acc: 0.7956\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 2s 306us/step - loss: 0.5664 - acc: 0.7964\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 2s 319us/step - loss: 0.5537 - acc: 0.7938\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 2s 326us/step - loss: 0.5545 - acc: 0.7944\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 2s 329us/step - loss: 0.5412 - acc: 0.7969\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 2s 341us/step - loss: 0.5448 - acc: 0.7962\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 351us/step - loss: 0.5536 - acc: 0.7935\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 354us/step - loss: 0.5602 - acc: 0.7956\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 2s 347us/step - loss: 0.5526 - acc: 0.7971\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 356us/step - loss: 0.5444 - acc: 0.7967\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 363us/step - loss: 0.5624 - acc: 0.7953\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 364us/step - loss: 0.5495 - acc: 0.7974\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 374us/step - loss: 0.6540 - acc: 0.7914\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 380us/step - loss: 0.5906 - acc: 0.7925\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 389us/step - loss: 0.5575 - acc: 0.7944\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 395us/step - loss: 0.5538 - acc: 0.7962\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 398us/step - loss: 0.5569 - acc: 0.7957\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 405us/step - loss: 0.5552 - acc: 0.7953\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 442us/step - loss: 0.5445 - acc: 0.7943\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 442us/step - loss: 0.5475 - acc: 0.7953\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 444us/step - loss: 0.5602 - acc: 0.7946\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 460us/step - loss: 0.5558 - acc: 0.7975\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 472us/step - loss: 0.5498 - acc: 0.7922\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 3s 479us/step - loss: 0.5558 - acc: 0.7944\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 492us/step - loss: 0.5485 - acc: 0.7969\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 497us/step - loss: 0.5525 - acc: 0.7942\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 520us/step - loss: 0.5923 - acc: 0.7942\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 530us/step - loss: 0.5465 - acc: 0.7956\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 518us/step - loss: 0.5368 - acc: 0.7971\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 512us/step - loss: 0.6537 - acc: 0.7943\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 530us/step - loss: 0.5579 - acc: 0.7956\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 524us/step - loss: 0.5603 - acc: 0.7960\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 534us/step - loss: 0.5551 - acc: 0.7922\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 544us/step - loss: 0.5552 - acc: 0.7935\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 573us/step - loss: 0.5504 - acc: 0.7969\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 572us/step - loss: 0.5651 - acc: 0.7947\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 573us/step - loss: 0.5660 - acc: 0.7957\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 571us/step - loss: 0.5495 - acc: 0.7961\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 569us/step - loss: 0.6176 - acc: 0.7946\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 577us/step - loss: 0.5726 - acc: 0.7936\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 587us/step - loss: 0.6056 - acc: 0.7928\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 587us/step - loss: 0.5627 - acc: 0.7940\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 601us/step - loss: 0.5688 - acc: 0.7937\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 605us/step - loss: 0.5759 - acc: 0.7944\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 4s 618us/step - loss: 0.5785 - acc: 0.7969\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 631us/step - loss: 0.5703 - acc: 0.7951\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 633us/step - loss: 0.5673 - acc: 0.7935\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 640us/step - loss: 0.5596 - acc: 0.7936\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 634us/step - loss: 0.5508 - acc: 0.7971\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 639us/step - loss: 0.5910 - acc: 0.7940\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 646us/step - loss: 0.6019 - acc: 0.7928\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 655us/step - loss: 0.5752 - acc: 0.7949\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 659us/step - loss: 0.5717 - acc: 0.7904\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 668us/step - loss: 0.5502 - acc: 0.7944\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 719us/step - loss: 0.5382 - acc: 0.7969\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 695us/step - loss: 0.5757 - acc: 0.7962\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 690us/step - loss: 0.5729 - acc: 0.7956\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 694us/step - loss: 0.5761 - acc: 0.7943\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 721us/step - loss: 0.6123 - acc: 0.7949\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 741us/step - loss: 0.5596 - acc: 0.7967\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 742us/step - loss: 0.5706 - acc: 0.7953\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 759us/step - loss: 0.5704 - acc: 0.7946\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 754us/step - loss: 0.5828 - acc: 0.7913\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 774us/step - loss: 0.5587 - acc: 0.7944\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 780us/step - loss: 0.5730 - acc: 0.7965\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 783us/step - loss: 0.5574 - acc: 0.7962\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 787us/step - loss: 0.5923 - acc: 0.7924\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 822us/step - loss: 0.5573 - acc: 0.7961\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 793us/step - loss: 0.5836 - acc: 0.7946\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 805us/step - loss: 0.5564 - acc: 0.7961\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 812us/step - loss: 0.5915 - acc: 0.7937\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 828us/step - loss: 0.5496 - acc: 0.7975\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 830us/step - loss: 0.5760 - acc: 0.7908\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 835us/step - loss: 0.5893 - acc: 0.7911\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 831us/step - loss: 0.5713 - acc: 0.7936\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 840us/step - loss: 0.5701 - acc: 0.7943\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 843us/step - loss: 0.5529 - acc: 0.7957\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 860us/step - loss: 0.5711 - acc: 0.7931\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 7s 875us/step - loss: 0.5349 - acc: 0.7959\n"
     ]
    }
   ],
   "source": [
    "#Parameter Tuning \n",
    "#Evaluation of ANN by k fold cross validaion\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import keras\n",
    "from keras.models import Sequential #to initialize the NN\n",
    "from keras.layers import  Dense #to generate the layers\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "    '''\n",
    "    LOCAL CLASSIFIER\n",
    "    for cross validation K times going to call this function \n",
    "    and it will return \n",
    "    '''\n",
    "    #Initialize the NN\n",
    "    classifier = Sequential() #to initialize the first layer\n",
    "\n",
    "    #Adding the input layer and the first hidden layer \n",
    "\n",
    "    #Hidden layer\n",
    "    #sigmoid for output layer \n",
    "    #Rectifier for Hidden layer\n",
    "    classifier.add(Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=11, units=6))\n",
    "    #how many nodes? No rules but (avg of number nodes input and output layer) #input+output/2  =(11+1)/2 = 6\n",
    "    #init  = initilizatin of values\n",
    "\n",
    "    #second hidden layer \n",
    "    classifier.add(Dense(units=6, activation=\"relu\", kernel_initializer=\"uniform\")) #netwrok knows how to connect so need of input_dim\n",
    "\n",
    "    #Add final-output layer \n",
    "    classifier.add(Dense(units=1, activation=\"sigmoid\", kernel_initializer=\"uniform\")) #netwrok knows how to connect so need of input_dim\n",
    "\n",
    "    #compile the ANN\n",
    "    classifier.compile(optimizer = optimizer, loss ='binary_crossentropy',metrics = ['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_classifier)\n",
    "\n",
    "parameters = {'batch_size' :[25,32],\n",
    "              'nb_epoch' : [100,500],\n",
    "              'optimizer' : ['adam','rmsprop']}\n",
    "\n",
    "#created object\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10) \n",
    "\n",
    "#fitting object\n",
    "grid_search = grid_search.fit(X_train,y_train)\n",
    "best_parameter =  grid_search.best_params_\n",
    "best_accuracy  =  grid_search.best_score_\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RhwnoxaEY2i2",
    "outputId": "9a3978cb-f48e-48fd-d4d3-d03e7e877fb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 25, 'nb_epoch': 100, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print(best_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iNLyuxgtcAW8",
    "outputId": "cef37128-7a0d-4a71-9883-32138059c51c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796\n"
     ]
    }
   ],
   "source": [
    "print(best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MctKcAaOcEnL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Untitled4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
