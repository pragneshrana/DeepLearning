{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BackPropogationVectorize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkS0NOxmfil5YMCAEk1a+8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragneshrana/DeepLearning/blob/master/TrainingFeedForwardNN/BackPropogationVectorize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXh0R7YOfIEo",
        "colab_type": "text"
      },
      "source": [
        "# Blame Game:\n",
        "\n",
        "- The parameter that affect the output of NN is weights and bias\n",
        "- Every weights and bias associated neuron won't take responcibility of making error, rather than they blame subsquent neuron's weight for causing such error (Typical company's hierarchy :))\n",
        "- Blaming process goes till last layer to first layer and everything gets corrected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvffZGeNg_Q1",
        "colab_type": "text"
      },
      "source": [
        "$\n",
        "\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial W_{111}}}_{\\text{Talk to the last weight directly}} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\hat y}\\frac{\\partial \\hat y}{\\partial a_3}}_{\\text{Talk to the previous hidden layer}}\n",
        "\\cdot\n",
        "\\underbrace{\\frac{\\partial \\hat a_3 }{\\partial h_2}\\frac{\\partial h_2 }{\\partial a_2}}_{\\text{Talk to the previous hidden layer}}\n",
        "\\cdot\n",
        "\\underbrace{\\frac{\\partial \\hat a_2 }{\\partial h_1}\\frac{\\partial h_1 }{\\partial a_1}}_{\\text{Talk to the previous hidden layer}}\n",
        "\\cdot\n",
        "\\underbrace{\\frac{\\partial \\hat a_1 }{\\partial W_{111}}}_{\\text{Talk to the Weight}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdyW4yjMRSZw",
        "colab_type": "text"
      },
      "source": [
        "# Output Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0P1WKovoIIz",
        "colab_type": "text"
      },
      "source": [
        "In short our interest is,\n",
        "\n",
        "$\\frac{\\partial\\mathcal{L}}{a_{L_i}}=\\frac{\\partial (-\\log{\\hat y})}{a_{L_i}}$\n",
        "\n",
        "where, L = $L^{th}$ layer,    i = $i^{th}$ in $L^{th}$ layer.\n",
        "\n",
        "$\\nabla_{a_3}\\mathcal{L} = \\begin{bmatrix} \\frac{\\partial\\mathcal{L}}{a_{31}} \\\\\\frac{\\partial\\mathcal{L}}{a_{32}} \\end{bmatrix} \\text{where, L=3,i={1,2}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lr8vH72vfMf",
        "colab_type": "text"
      },
      "source": [
        "As, $\\hat y $ depends on the $a_3$\n",
        "\n",
        "$\\hat y = \\frac{\\exp a_l}{\\sum_i \\exp a_{l_i}}$\n",
        "\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial a_{L_i}}=\\frac{\\partial (-\\log{\\hat y_l})}{\\partial \\hat y_l}\\frac{\\partial \\hat y_l}{\\partial a_{L_i}}$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0lwmYA30vrt",
        "colab_type": "text"
      },
      "source": [
        "$ \\frac{\\partial (-\\log{\\hat y_l})}{\\partial \\hat y_l}\\\\\n",
        "= -\\frac{1}{\\hat y_l} \\frac{\\partial}{\\partial a_{L_i}}{\\hat y_l}\\\\\n",
        "= -\\frac{1}{\\hat y_l} \\frac{\\partial}{\\partial a_{L_i}}{softmax(a_L)_l}  \\text{, where $a_l$ is vector}\\\\\n",
        "=-\\frac{1}{\\hat y} \\frac{\\partial}{\\partial a_{L_i}}{\\frac{\\exp(a_L)_l}{\\sum_{i'}\\exp(a_L)_{i'}}}\n",
        "$\n",
        "\n",
        "Rule:\n",
        "$\n",
        "\\frac{\\partial \\frac{g(x)}{h(x)}}{\\partial x} =  \\frac{\\partial g(x)}{\\partial x}\\frac{1}{h(x)} -\\frac{g(x)}{h(x)^2} \\frac{\\partial h(x)}{\\partial x}\n",
        "$\n",
        "\n",
        "$=-\\frac{1}{\\hat y} \\Bigg({\\frac{\\frac{\\partial}{\\partial a_{L_i}}\\exp(a_L)_l}{\\sum_{i'}\\exp(a_L)_{i'}}} - \\frac{\\exp(a_L)_l\\frac{\\partial \\sum_{i'}\\exp(a_L)_{i'}}{\\partial a_{L_i}}}{\\bigg(\\sum_{i'}\\exp(a_L)_{i'}\\bigg)^2} \\Bigg)\n",
        "$\n",
        "\n",
        "$=-\\frac{1}{\\hat y} \\Bigg({\\frac{\\mathbb{1}_{l=i}\\exp(a_L)_{l}}{\\sum_{i'}\\exp(a_L)_{i'}}} - \\frac{\\exp(a_L)_l}{{\\sum_{i'}\\exp(a_L)_{i'}}}  \\frac{\\exp(a_L)_l}{\\sum_{i'}\\exp(a_L)_{i'}}\\Bigg)\n",
        "$\n",
        "\n",
        "$=-\\frac{1}{\\hat y}(\\mathbb{1}_{l=i} softmax(a_L)_l - softmax(a_L)_l softmax(a_L)_i)$\n",
        "\n",
        "$=-\\frac{1}{\\hat y}(\\mathbb{1}_{l=i} \\hat y_l - \\hat y_l \\hat y_i)$\n",
        "\n",
        "$=-(\\mathbb{1}_{l=i} - \\hat y_i)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6pG6ymGEv2N",
        "colab_type": "text"
      },
      "source": [
        "$\\nabla_{a_L}\\mathcal{L} = \\begin{bmatrix} \\frac{\\partial\\mathcal{L}}{a_{L1}} \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial\\mathcal{L}}{a_{L2}} \\end{bmatrix} = - (e(L) - \\hat y), \\text{where e(L) is one hot encoder [0 0 0 ðŸ”¥ 0 0]}$ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-pCkKp5RLtU",
        "colab_type": "text"
      },
      "source": [
        "## Hidden Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myoUpJr_RXwV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c61jOBTRRWpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}