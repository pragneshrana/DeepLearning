{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ActivationFunctionInitializationMethod.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3FAb41RI+RzeP22aLsOLy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragneshrana/DeepLearning/blob/master/TrainingFeedForwardNN/ActivationFunctionInitializationMethod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3etqNZhSghj",
        "colab_type": "text"
      },
      "source": [
        "**What if there is no activation function?**\n",
        " - $\\hat y = W_3(W_2(W_1 \\cdot x)) = W \\cdot x$ which is just a linear transformation. \n",
        " - UAT does not hold\n",
        "\n",
        "# Popular Activation function\n",
        "  - tanh\n",
        "  - logistic \n",
        "  - ReLU\n",
        "  - leaky ReLU\n",
        "\n",
        "# Logistic/Sigmoid function\n",
        "\n",
        " - $f = \\frac{1}{1+\\exp^{-x}} \\& f' = f (1-f) $\n",
        "\n",
        "# Vanishing graient problem (Saturation)\n",
        "  - when \n",
        "    - $f(x) = 0 or  1  $\n",
        "    - $ f'(x) = 0$\n",
        "  - $\\nabla W = 0$\n",
        "  - $W_{new} = W_{old}+ \\eta \\nabla W \\rightarrow W_{new} = W_{old}$\n",
        "  - so, gradient won't move and stagnation will happen\n",
        "\n",
        "  - $\\sigma(\\sum_i w_ix_i) \\rightarrow BLOWUP $ if $w_ix_i > \\pm 5$\n",
        "\n",
        "  - **Initialize weight to smaller number**\n",
        "  \n",
        "\n",
        "# Issues with logistic function\n",
        "   - Non-zero centered (Gradient can move in positive or negative -- all gradients have same sign so, 1st or 3rd quadrant) :(\n",
        "   - Vanishing gradient :(\n",
        "   - Computationally constly to calculate :(\n",
        "  \n",
        "  # Tanh \n",
        "   - $f  = \\frac{\\exp^x  - \\exp^{-x}}{\\exp^x  + \\exp^{-x}} \\in \\{-1,1\\}  \\& f'= 1-f^2$ \n",
        "\n",
        "   - zero centered (Gradient can move in all quadrant) :)\n",
        "\n",
        "   # ReLU\n",
        "   - $f = max(0,1)$\n",
        "\n",
        "      - $f' : \\frac{\\partial f}{\\partial x} = 0 \\leftarrow x<0$\n",
        "      - $ f': \\frac{\\partial f}{\\partial x} = 1 \\leftarrow x>0$\n",
        "\n",
        "    - Easy to compute :)\n",
        "    - Avoid saturation in positive region :)\n",
        "\n",
        "- Problem with ReLU :\n",
        "  - if $w_1x_2 + w_2x_2 + b << 0 \\rightarrow max\\{0,\\sum_i w_ix_i+b\\} = 0$\n",
        "  - h1 = 0 \n",
        "  - $\\frac{\\partial h_1}{\\partial a_1} = 0$ Dead Neuron\n",
        "  - Once dead, Always dead\n",
        "\n",
        "- Set Bias to positive\n",
        "\n",
        "# Leaky ReLU\n",
        "\n",
        "   - $f = max(0.01x,x)$\n",
        "\n",
        "      - $f' : \\frac{\\partial f}{\\partial x} = 0.01 \\leftarrow x<0$\n",
        "      - $ f': \\frac{\\partial f}{\\partial x} = 1 \\leftarrow x>0$\n",
        "\n",
        "    - Does not saturate in positive and negative region :)\n",
        "    - will not die :)\n",
        "    - Close to zero centered :)\n",
        "    - easy to compute :)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o_ZsAsaveRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}