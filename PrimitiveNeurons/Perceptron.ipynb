{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZsVPZ/I9cXinvjvHW5xpg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragneshrana/DeepLearning/blob/master/PrimitiveNeurons/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoZyjRiZFVYO",
        "colab_type": "text"
      },
      "source": [
        "# Perceptron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnSUkW8Tg-om",
        "colab_type": "text"
      },
      "source": [
        "**Model**:\\\n",
        "Peceptron model is motivated from the concept of MP Neuron.\n",
        "\n",
        "**Data and Task**:\n",
        "- As is discussed before,, for example data for mobile phone is given below.\n",
        "\n",
        "$\\begin{matrix}\n",
        "   & M1 & M2 & M3 & M4 \\\\\n",
        "  weight & 151 & 181  & 160 & 148 \\\\\n",
        "  dual sim & 1 &  1 & 1 & 0 \\\\\n",
        "  NFC & 1 & 0 & 1 & 1 \\\\\n",
        "  Battery & 3060 & 5000& 4500 & 4000 \\\\\n",
        "  Price & 15000 & 20000 & 17000 & 16000 \\\\\n",
        "  Like & 1 & 0 & 1 & 1\\\\\n",
        "\\end{matrix}$\n",
        "\n",
        "- Now, as data varies a lot in comparision of rows, it may affect the result as data with higher range can have unnatural advantage over other which can affect the weights. so, there is a need of standardization.\n",
        "\n",
        "- standardization of the data can be done by, \n",
        "$\\begin{align}x' = \\frac{x- x_{min}}{x_{max} - x_{min}}\\end{align}$\n",
        "\n",
        "- After standardization, data will be,\n",
        "$\\begin{matrix}\n",
        "   & M1 & M2 & M3 & M4 \\\\\n",
        "  weight & 0.09 & 1  & 0.36 & 0 \\\\\n",
        "  dual sim & 1 &  1 & 1 & 0 \\\\\n",
        "  NFC & 1 & 0 & 1 & 1 \\\\\n",
        "  Battery & 0 & 1& 0.74 & 0.48 \\\\\n",
        "  Price & 0 & 1 & 0.4 & 0.2 \\\\\n",
        "  Like & 1 & 0 & 1 & 1\\\\\n",
        "\\end{matrix}$\n",
        "\n",
        "**Perceptron Model**: \\\n",
        "- Difference between perceptron model and MP neuron? \\\n",
        "It has real inputs and weight associated with every input. \\\n",
        "\n",
        " Model: \\\n",
        "\n",
        "$y = \\begin{cases}\n",
        "1 & \\text{if $\\sum_i w_ix_i \\ge$  b}\\\\\n",
        "0 & \\text{ otherwise}\n",
        "\\end{cases} $\n",
        "\n",
        "- Weight associated with features gives importance about the feature.\n",
        "\n",
        "For M3, \\\n",
        "\n",
        "$\\vec x = [0.36, 1 , 1 , 0.74, 0.4] \\in \\mathcal{R^5}$ \\\n",
        "$\\vec w = [-0.3 , 0.4, -0.1 , 0.6, 0.3] \\in \\mathcal{R^5}$\n",
        "\n",
        "now, $w^T x$ can give the value of $y_{predict}$\n",
        "\n",
        "**Geometric Interpretation:** \\\n",
        "Geometric Interpetation is same as described ahead.\n",
        "\n",
        "**Loss Function:** \\\n",
        "- What is the purpose of the loss function? \\\n",
        "  To tell the model that some correction needs to be done. \n",
        "\n",
        "- loss function $\\mathcal L = \\begin{cases}\n",
        "0 & \\text{if $ y = \\hat y$  }\\\\\n",
        "1 & \\text{ otherwise}\n",
        "\\end{cases} $\n",
        "\n",
        "- Perceptron loss = $\\mathcal 1 _{y - \\hat y}$\n",
        "- Squared Error loss = $(y - \\hat y)^2$\n",
        "- Squared error loss is equivalennt to perceptron loss when the outut is boolean.\n",
        "\n",
        "**General Learning ALgorithm :**\n",
        "\n",
        "1. Initialize $w_1,w_1,b$\n",
        "2. Iterate over the data :\n",
        "    - $y = f(x_1,x_2)$\n",
        "    - $\\mathcal L $ = compute loss$(x_i)$\n",
        "    - if not satisfied: \n",
        "        - update$(w_1,w_1,b)$\n",
        "        - Go to step-2\n",
        "\n",
        "**Perceptron Learning Algorithm :**\n",
        "1. P $\\rightarrow$ inputs with label 1\n",
        "2. N $\\rightarrow$ inputs with label 0\n",
        "3. Initialize **w** randomly\n",
        "4. While !convergence do\n",
        "    - Pick random **x** $\\in P \\cup N$\n",
        "    - if **x** $\\in P$ and $\\sum_i w_i x_i < 0 $ then\n",
        "        - w = w + x\n",
        "    - if **x** $\\in N$ and $\\sum_i w_i x_i \\ge 0 $ then\n",
        "        - w = w - x\n",
        "\n",
        "Algorithm converges when all inputs are classified correctly.\n",
        "\n",
        "**Why this alorithm works?**\n",
        "\n",
        "1. Basics:\n",
        " - $\\cos \\theta = \\frac{w \\cdot x}{||w|| ||x||}$ and $-1 \\le  \\cos \\theta \\le 1$\n",
        " - Based on the it can decide that weather plane makes obtuse angle  $(90<\\theta<180)$ or actue angle $(<90)$\n",
        "\n",
        "2. In perceptron the $\\cos \\theta$ depends only on the dot procuct of the $w$ and $x$.\n",
        "- For x $\\in$ P if $w\\cdot x < 0$ then it means that the angle$(\\alpha)$ between this x and the current w is greater than 90 (we want lesser) \n",
        "  - $w_{new} = w + x$\n",
        "\n",
        "- $\\cos(\\alpha_{new}) \\propto w_{new}^T x$\n",
        "  - $\\propto (w+x)^T x$\n",
        "  - $\\propto w^T x + x^T x$\n",
        "  - $\\propto \\cos \\alpha + x^T x$\n",
        "\n",
        "- Similar can be written for the x $\\in$ N if $w\\cdot x > 0$\n",
        "\n",
        "- This algorithm will only work for those data which are linealy seperable.\n",
        "\n",
        "**Def:** \\\n",
        "Two sets P and N of points in an n-dimensional space are called sbsolutely linealry separable if n+1 real numbers $w_0, w_1, ..., w_n $ exist such that every point $(x_0, x_1, ..., x_n) \\in P$ satisfies $\\sum_i w_i x_i > w_0$ and every point $(x_0, x_1, ..., x_n) \\in N$ satisfies $\\sum_i w_i x_i < w_0$.\n",
        "\n",
        "**Proposition:**\n",
        "If set P and N are finite and linearly seperable learning algorthm will converge in a finite number of steps.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSNepIBBgxoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}