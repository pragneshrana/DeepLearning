{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProbabilityTheory.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTOarg5INtHfD4+TFRrQwt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragneshrana/DeepLearning/blob/master/SigmoidNeuron/Probability%26InformationTheory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSgms6Gl-y1G",
        "colab_type": "text"
      },
      "source": [
        "# Intro :\n",
        "\n",
        "- For any event A, P(A) $\\ge$ 1\n",
        "- For disjoint event $A_1, A_2, A_3, ... $and $A_i \\cap A_j = \\phi$ then $P(\\cup A_i) = \\sum_i P_i$\n",
        "- P(All events $\\Omega$) = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjcPebEMEgu6",
        "colab_type": "text"
      },
      "source": [
        "# Random Variable\n",
        "\n",
        "- Random variable is function which maps the input to ontput or particular attribute. $P(G=g)$ where $g \\in {A,B,C}$\n",
        "\n",
        "or \n",
        "\n",
        "- Randomm variable is function which maps the each outcome to the value\n",
        "\n",
        "- It might be,\n",
        "  - continious\n",
        "  - discrete\n",
        "\n",
        "- What is distrbution (Marginal Distrbution)?\n",
        "  - Distrbution of individual random variable is called marginal variable.xv\n",
        "  - Consider a random varibel G can take any value $G \\in {A,B,C}$\n",
        "\n",
        ">G|P(G=g)\n",
        ">---|---\n",
        ">A | 0.1\n",
        ">B | 0.2\n",
        ">C | 0.7\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGEd7YIME5dN",
        "colab_type": "text"
      },
      "source": [
        "# True vs Predicted distrbution\n",
        "\n",
        "- True dist. = [0.25 0.5 0.2 0.05]\n",
        "- Predicted dist. = [0.2 0.6 0.1 0.1]\n",
        "\n",
        "One way to evalate performace between true or predicted distribution is using MSE $\\sum_i (\\hat y_i - y_i) $.\n",
        "\n",
        "- True output and Predicted output of result is actually distribution.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubo7TJvZdsqA",
        "colab_type": "text"
      },
      "source": [
        "# Expectation:\n",
        "\n",
        "What is expectation of distrbution?\n",
        "- Consider the random variable X which maps the winning team amongst the 4 team. P(X)$\\in {A,B,C,D}.$\n",
        "- G(X=x) is associated with each of team for win.\n",
        "- **Expected value or expectation** is weighted average.\n",
        "- A random variable can take any value and each value has probability associated with it so, weighted average value denotes expected value or expectation.\n",
        "\n",
        "$E(x) = \\sum_{A,B,C,D} P(X=i) * G(X=i) $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq4DJYRpg-sC",
        "colab_type": "text"
      },
      "source": [
        "# Information Content (IC)\n",
        "\n",
        "Consider random variable X, P(X=x) denotes weather there will be strom or not. It is given that P(X=x)=0.2\n",
        "which is quite low so if strom occurs it is surprising as information gained is very high given very less probability as this is surprising event. \n",
        "\n",
        "$\\text{Information content} \\propto Surprice \\propto \\frac{1}{P(X=sample)}$\n",
        "\n",
        "So, it can inferred that information content is function of probability. $IC(P(X=S))$\n",
        "\n",
        "\n",
        "**To find out functional form consider and example:**\n",
        "\n",
        "Consider two event Cricket team to win match :{A,B,C,D}  & Light switch :{on , off}\n",
        "\n",
        "As, this events are independent, if it is given that Team-B win the match and lights are on\n",
        "then,\n",
        "\n",
        "Information gained,\n",
        "$IC(X=B \\cap Y=on) = IC(X=B) + IC(Y=on)$ --\n",
        "as both events are not dependent so total information gained is sum of information gained by this two function.\n",
        "\n",
        "**Summing up,**\\\n",
        "Information content is function of probability and for independent event it follows summation rule.\n",
        "\n",
        "$\n",
        "IC(P(X=S)) = \\text{Information content is function of probability}$\n",
        "$IC(P(X\\cap Y)) = IC(P(X)) + IC(P(Y)) $\n",
        "\n",
        "For disjoint events, $P(X\\cap Y) = P(X) \\dot P(Y)$\n",
        "\n",
        "$IC(P(X\\cap Y)) = IC(P(X) \\dot P(Y)) = IC(P(X)) + IC(P(Y))$\n",
        "\n",
        "Family of function that satisfy $f(a \\cdot b) = f(a) + f(b)$ is $\\log(a \\cdot b) = log(a) + log(b)$\n",
        "\n",
        "- IC function follows, $IC(X=A) =\\log(\\frac{1}{P(X=A)}) =\\log(1) - \\log(P(X=A)) = -log_2(X=A) $\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF9FGqzgmbqD",
        "colab_type": "text"
      },
      "source": [
        "# Entropy \n",
        "\n",
        "Entropy of random variable is obtained by expected gain.\n",
        "\n",
        "$E(\\text{Gain}) = \\sum_{A,B,C,D} P(x=i) Gain(X=i)$\n",
        "\n",
        "Gain can be mimic by Information content so,\\\n",
        "Entropy $H(x) = - \\sum_{i \\in {A,B,C,D}} P(X=i) \\log_2 P(X=i) $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZwEJ2cgUxj_",
        "colab_type": "text"
      },
      "source": [
        "# Relation to bits and entropy\n",
        "\n",
        "To pass any message for {A,B,C,D} two bits are required for each message as {00,01,10,11}. Assume Assoaciated probability is {0.25 0.25 0.25 0.25}.\n",
        "\n",
        "so, Information content associated with each of thing is, $\\log_2(1/4)= 2$ which shows that **number bits required to transmit a message is equal to Information content.**\n",
        "\n",
        "In reality,\n",
        "> -      | A | B | C | D|\n",
        ">--------|---|---|---|---\n",
        ">P(X)   |0.5|0.25|0.125|0.125\n",
        "> IC |1|2|3| 3 \n",
        "\n",
        "where Information content is calculated by $- \\log(P)$. Here, Message by A is more frequent so, if less bits are spend to send message can reduce the average entropy $H(x) = 0.5*1+0.25*2+0.125*3+0.125*3 = 1.75 $ which is lesser than previous case.\n",
        "\n",
        "So, Entropy gives expectation of bits required to transmit a mesasge. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLZxaHmjWzS3",
        "colab_type": "text"
      },
      "source": [
        "# KL divergence and Cross Entropy\n",
        "\n",
        "Entropy = $-\\sum_i y_i \\log y_i$ \\\n",
        "\n",
        "Cross Entropy = $-\\sum_i y_i \\log \\hat y_i$\n",
        "\n",
        "where, $y =$Actual value & $\\hat y$ = Predicted value\n",
        "\n",
        "The realationn between entropy and cross entropy given KL-Divergence,\n",
        "\n",
        "KLD = $-\\sum_i y_i \\log \\hat y_i + \\sum_i y_i \\log y_i$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPsmdYFIh7nl",
        "colab_type": "text"
      },
      "source": [
        "- The objective is to minimize the error so, \\\n",
        "min KLD = $\\min (-\\sum_i y_i \\log \\hat y_i + \\sum_i y_i \\log y_i) \\approx \\min f(w,b)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiLDeN5CkmP1",
        "colab_type": "text"
      },
      "source": [
        "# What does the cross entropy function looks like?\n",
        "\n",
        "$y = [0,1]$\\\n",
        "$\\hat y = [0.7,0.3]$\n",
        "\n",
        "Now,\n",
        "loss function can be defined as,\n",
        "\n",
        "$\\mathcal{L} = -\\sum_{i_{0,1}} y_i \\log\\hat y_i$\n",
        "\n",
        "$\\mathcal{L} = y_0 \\log \\hat y_0 + y_1 \\log \\hat y_1$\n",
        "\n",
        "$\\mathcal{L} = y_0 \\log (1-\\hat y_1)+ y_1 \\log \\hat y_1$\n",
        "\n",
        "General form,\n",
        "$\\mathcal{L} = (1-y) \\log (1-\\hat y)+ y\\log \\hat y$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aklHk_TMzn50",
        "colab_type": "text"
      },
      "source": [
        "# Algorithm:\n",
        "\n",
        "- Initialize w,b\n",
        "- Iterate over data\n",
        "    - compute $\\hat y$\n",
        "    - compute $\\mathcal{L(w,b)}$\n",
        "    - $ w_{t+1} = w_t + \\eta \\Delta w_t $\n",
        "    - $ b_{t+1} = b_t + \\eta \\Delta b_t $\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoODtsSI2cLR",
        "colab_type": "text"
      },
      "source": [
        "# Calculation of Gradient:\n",
        "\n",
        "$\\mathcal{L} = (1-y) \\log (1-\\hat y)+ y\\log \\hat y$\n",
        "\n",
        "now,\n",
        "$\\Delta w = \\frac{ \\partial \\mathcal{L}}{\\partial w} = \\frac{ \\partial \\mathcal{L}}{\\partial \\hat y} \\frac{ \\partial \\mathcal{\\hat y}}{\\partial w}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RnbGyrAEkl0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}