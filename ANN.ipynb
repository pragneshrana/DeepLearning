{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragneshrana/DeepLearning/blob/master/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHxysLQBfcg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Artificial Neural Network\n",
        "import pandas as pd\n",
        "url = 'https://raw.githubusercontent.com/pragneshrana/DeepLearning/master/Artificial_Neural_Networks/Churn_Modelling.csv'\n",
        "dataset = pd.read_csv(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiCDHY28gJK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing the dataset \n",
        "X = dataset.iloc[:,3:13].values\n",
        "y = dataset.iloc[:,13].values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZyqvKNzZR0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoding the variable\n",
        "#label  encoder converts the categoical data into numeric data with same number of categories\n",
        "#whereas onehot encoder generated number of rows by taking combination of rows.\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import numpy as np\n",
        "labelencoder_X_1 = LabelEncoder()\n",
        "X[:,1] = labelencoder_X_1.fit_transform(X[:,1])\n",
        "labelencoder_X_2 = LabelEncoder()\n",
        "X[:,2] = labelencoder_X_2.fit_transform(X[:,2])\n",
        "ct = ColumnTransformer(\n",
        "    [('one_hot_encoder', #name\n",
        "      OneHotEncoder(), #TransformClass\n",
        "      [0])],    #index to trnasform\n",
        "    remainder='passthrough'                         # Leave the rest of the columns untouched\n",
        ")\n",
        "X = np.array(ct.fit_transform(X), dtype=np.float)\n",
        "X = X[:,1:] #REMOVED FIRST COLM\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OVOM8FiZYdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train Test set \n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzTcS4N1c0qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#feature scaling \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test  = sc.transform(X_test)\n",
        "\n",
        "#TO see the diffn\n",
        "# scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "# scaler.transform(X_train) \n",
        "# scaler.transform(X_test) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZWJhVF2flL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Keras and Implementation of ANN\n",
        "import keras\n",
        "from keras.models import Sequential #to initialize the NN\n",
        "from keras.layers import  Dense #to generate the layers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXiqBbgXgZjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize the NN\n",
        "classifier = Sequential() #to initialize the first layer\n",
        "\n",
        "#Adding the input layer and the first hidden layer \n",
        "\n",
        "#Hidden layer\n",
        "#sigmoid for output layer \n",
        "#Rectifier for Hidden layer\n",
        "classifier.add(Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=11, units=6))\n",
        "#how many nodes? No rules but (avg of number nodes input and output layer) #input+output/2  =(11+1)/2 = 6\n",
        "#init  = initilizatin of values\n",
        "\n",
        "#second hidden layer \n",
        "classifier.add(Dense(units=6, activation=\"relu\", kernel_initializer=\"uniform\")) #netwrok knows how to connect so need of input_dim\n",
        "\n",
        "#Add final-output layer \n",
        "classifier.add(Dense(units=1, activation=\"sigmoid\", kernel_initializer=\"uniform\")) #netwrok knows how to connect so need of input_dim\n",
        "\n",
        "#compile the ANN\n",
        "classifier.compile(optimizer ='adam', loss ='binary_crossentropy',metrics = ['accuracy'])\n",
        "#adam is stochastic gradient method"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEvipYFbhZE3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52b45979-7b7a-4aa8-c7a0-bbde8de5f50d"
      },
      "source": [
        "#Fit model to training set\n",
        "classifier.fit(X_train,y_train,batch_size = 10,nb_epoch = 100)\n",
        "# 3. Update weights \n",
        "# 4. num of epochs                "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1330/8000 [===>..........................] - ETA: 0s - loss: 0.3154 - acc: 0.8789"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3378 - acc: 0.8625\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3382 - acc: 0.8611\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.3380 - acc: 0.8626\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.3379 - acc: 0.8616\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3380 - acc: 0.8625\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.3378 - acc: 0.8620\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 1s 116us/step - loss: 0.3375 - acc: 0.8601\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3377 - acc: 0.8642\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3372 - acc: 0.8617\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3373 - acc: 0.8622\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3381 - acc: 0.8611\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3373 - acc: 0.8627\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3373 - acc: 0.8626\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3375 - acc: 0.8635\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3374 - acc: 0.8630\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.3377 - acc: 0.8630\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3376 - acc: 0.8617\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3376 - acc: 0.8609\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3372 - acc: 0.8636\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3372 - acc: 0.8630\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3372 - acc: 0.8642\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3378 - acc: 0.8615\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3372 - acc: 0.8619\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3375 - acc: 0.8619\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3369 - acc: 0.8639\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3385 - acc: 0.8612\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3370 - acc: 0.8630\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3380 - acc: 0.8607\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3376 - acc: 0.8637\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3371 - acc: 0.8619\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3377 - acc: 0.8630\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.3367 - acc: 0.8634\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3381 - acc: 0.8617\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 1s 117us/step - loss: 0.3377 - acc: 0.8619\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3376 - acc: 0.8634\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3375 - acc: 0.8624\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3375 - acc: 0.8634\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3370 - acc: 0.8612\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3375 - acc: 0.8630\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 1s 116us/step - loss: 0.3373 - acc: 0.8619\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3375 - acc: 0.8607\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3376 - acc: 0.8622\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3375 - acc: 0.8622\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3375 - acc: 0.8635\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 1s 116us/step - loss: 0.3370 - acc: 0.8610\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 1s 117us/step - loss: 0.3376 - acc: 0.8627\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3371 - acc: 0.8627\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3371 - acc: 0.8625\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3369 - acc: 0.8607\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3367 - acc: 0.8612\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3366 - acc: 0.8624\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3370 - acc: 0.8609\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3364 - acc: 0.8627\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3363 - acc: 0.8631\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3371 - acc: 0.8621\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.3362 - acc: 0.8627\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3358 - acc: 0.8606\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 1s 118us/step - loss: 0.3356 - acc: 0.8621\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3358 - acc: 0.8627\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3350 - acc: 0.8621\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3354 - acc: 0.8621\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3352 - acc: 0.8617\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3352 - acc: 0.8615\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 1s 109us/step - loss: 0.3350 - acc: 0.8629\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3351 - acc: 0.8626\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3351 - acc: 0.8621\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3350 - acc: 0.8622\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3347 - acc: 0.8639\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3348 - acc: 0.8617\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3352 - acc: 0.8632\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3345 - acc: 0.8615\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 1s 141us/step - loss: 0.3346 - acc: 0.8610\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 1s 158us/step - loss: 0.3343 - acc: 0.8609\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 1s 130us/step - loss: 0.3346 - acc: 0.8636\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.3344 - acc: 0.8627\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3349 - acc: 0.8624\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 1s 116us/step - loss: 0.3338 - acc: 0.8645\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3344 - acc: 0.8624\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3346 - acc: 0.8615\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 1s 116us/step - loss: 0.3338 - acc: 0.8645\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3342 - acc: 0.8632\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3347 - acc: 0.8641\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3341 - acc: 0.8614\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3345 - acc: 0.8630\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.3338 - acc: 0.8634\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3342 - acc: 0.8634\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3341 - acc: 0.8651\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3342 - acc: 0.8640\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3334 - acc: 0.8620\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3344 - acc: 0.8642\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3337 - acc: 0.8636\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3330 - acc: 0.8649\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3337 - acc: 0.8641\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3344 - acc: 0.8636\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3340 - acc: 0.8630\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3337 - acc: 0.8644\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3333 - acc: 0.8626\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3333 - acc: 0.8614\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3337 - acc: 0.8604\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3334 - acc: 0.8631\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa70e8f6490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWFIksB5qgTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predicction\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXvYVX3frSta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "80231cbd-9f82-43a1-b8d5-e235f28f399b"
      },
      "source": [
        "#Making the confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "print(cm)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[625 970]\n",
            " [190 215]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ5iwY6BsATM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}